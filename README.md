
# Local mutlimodal RAG using OLLAMA

This is a local multimodal RAG deployed using streamlit that can take in both URLs and local PDFs as input for querying.
Best part? Use your preferred open source models as well as embeddings!





## Run Locally

Clone the project

```bash
  git clone https://github.com/ungodlike/Multimodal-local-RAG-using-Ollama
```

Go to localrag.py, open terminal and create a virtual environment

```bash
  python -m venv env
```

Install dependencies

```bash
  pip install -r requirements.txt
```

Start the server

```bash
  streamlit run localrag.py
```


## Contributing

Contributions are always welcome!

Currently working on:

1. Integrating GPU layers for faster run time

2. Improving UI

3. Integrating LLAVA 

4. Integrating Whisper AI




## ðŸ”— Links
[![portfolio](https://img.shields.io/badge/my_portfolio-000?style=for-the-badge&logo=ko-fi&logoColor=white)](https://shhahzaans-ai-portfolio.vercel.app/)
[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/shhahzaan-khan)


